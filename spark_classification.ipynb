{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark as fds\n",
    "fds.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DS_Final_Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+--------------+----+--------------+--------------+-------------+---------------+--------------------+-----------+-------+---------+------------+------------------+\n",
      "|_c0|Snapshot ID| Snapshot Date|  Checkin Date|Days|Original Price|Discount Price|Discount Code|Available Rooms|          Hotel Name|Hotel Stars|DayDiff|  WeekDay|DiscountDiff|      DiscountPerc|\n",
      "+---+-----------+--------------+--------------+----+--------------+--------------+-------------+---------------+--------------------+-----------+-------+---------+------------+------------------+\n",
      "|  0|          1|7/17/2015 0:00|8/12/2015 0:00|   5|          1178|          1040|            1|              6|Best Western Plus...|          3|     26|Wednesday|         138|11.714770797962649|\n",
      "|  1|          1|7/17/2015 0:00|8/19/2015 0:00|   5|          1113|           982|            1|              8|Best Western Plus...|          3|     33|Wednesday|         131|11.769991015274034|\n",
      "|  2|          1|7/17/2015 0:00|8/13/2015 0:00|   5|          4370|          4240|            1|              3|The Peninsula New...|          5|     27| Thursday|         130|2.9748283752860414|\n",
      "|  3|          1|7/17/2015 0:00|7/26/2015 0:00|   5|          1739|          1667|            1|             18|Eventi Hotel a Ki...|          4|      9|   Sunday|          72| 4.140310523289246|\n",
      "|  4|          1|7/17/2015 0:00|8/12/2015 0:00|   5|          1739|          1672|            1|              3|Eventi Hotel a Ki...|          4|     26|Wednesday|          67|3.8527889591719378|\n",
      "|  5|          1|7/17/2015 0:00| 8/7/2015 0:00|   5|          1436|          1345|            1|              9|Grand Hyatt New York|          4|     21|   Friday|          91| 6.337047353760446|\n",
      "|  6|          1|7/17/2015 0:00| 8/9/2015 0:00|   5|          1616|          1525|            1|              5|Grand Hyatt New York|          4|     23|   Sunday|          91| 5.631188118811881|\n",
      "|  7|          1|7/17/2015 0:00|8/12/2015 0:00|   5|          1256|          1183|            1|              5|Grand Hyatt New York|          4|     26|Wednesday|          73| 5.812101910828026|\n",
      "|  8|          1|7/17/2015 0:00|8/13/2015 0:00|   5|          1256|          1201|            1|             54|Grand Hyatt New York|          4|     27| Thursday|          55| 4.378980891719745|\n",
      "|  9|          1|7/17/2015 0:00|7/22/2015 0:00|   5|          1689|          1489|            1|             -1|Hilton New York F...|          4|      5|Wednesday|         200|11.841326228537596|\n",
      "| 10|          1|7/17/2015 0:00|7/30/2015 0:00|   5|          2098|          2003|            1|             -1|DoubleTree Suites...|          4|     13| Thursday|          95| 4.528122020972355|\n",
      "| 11|          1|7/17/2015 0:00|7/31/2015 0:00|   5|          2120|          2026|            1|             -1|DoubleTree Suites...|          4|     14|   Friday|          94| 4.433962264150943|\n",
      "| 12|          1|7/17/2015 0:00|7/26/2015 0:00|   5|          1665|          1595|            1|             -1|Hampton Inn Manha...|          3|      9|   Sunday|          70|4.2042042042042045|\n",
      "| 13|          1|7/17/2015 0:00| 8/2/2015 0:00|   5|          1665|          1595|            1|             -1|Hampton Inn Manha...|          3|     16|   Sunday|          70|4.2042042042042045|\n",
      "| 14|          1|7/17/2015 0:00|7/22/2015 0:00|   5|          3900|          3750|            1|              8| Park Hyatt New York|          5|      5|Wednesday|         150|3.8461538461538463|\n",
      "| 15|          1|7/17/2015 0:00|7/23/2015 0:00|   5|          3900|          3750|            1|              8| Park Hyatt New York|          5|      6| Thursday|         150|3.8461538461538463|\n",
      "| 16|          1|7/17/2015 0:00|7/24/2015 0:00|   5|          3900|          3750|            1|              9| Park Hyatt New York|          5|      7|   Friday|         150|3.8461538461538463|\n",
      "| 17|          1|7/17/2015 0:00|7/25/2015 0:00|   5|          3900|          3750|            1|              9| Park Hyatt New York|          5|      8| Saturday|         150|3.8461538461538463|\n",
      "| 18|          1|7/17/2015 0:00|7/26/2015 0:00|   5|          3900|          3750|            1|              9| Park Hyatt New York|          5|      9|   Sunday|         150|3.8461538461538463|\n",
      "| 19|          1|7/17/2015 0:00|7/27/2015 0:00|   5|          3900|          3750|            1|              9| Park Hyatt New York|          5|     10|   Monday|         150|3.8461538461538463|\n",
      "+---+-----------+--------------+--------------+----+--------------+--------------+-------------+---------------+--------------------+-----------+-------+---------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hotels = spark.read.csv('Hotels_data_Changed.csv',inferSchema=True,header=True)\n",
    "\n",
    "hotels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-------+--------------------+---------+-------------+\n",
      "|  Snapshot Date|   Checkin Date|DayDiff|          Hotel Name|  WeekDay|Discount Code|\n",
      "+---------------+---------------+-------+--------------------+---------+-------------+\n",
      "|  1/1/2016 0:00| 1/13/2016 0:00|     12|Cassa Hotel 45th ...|Wednesday|            1|\n",
      "|  1/1/2016 0:00| 1/13/2016 0:00|     12|The New York EDITION|Wednesday|            3|\n",
      "|  1/1/2016 0:00| 1/29/2016 0:00|     28|Hampton Inn Manha...|   Friday|            4|\n",
      "| 10/1/2015 0:00| 10/2/2015 0:00|      1|Hilton Garden Inn...|   Friday|            4|\n",
      "|10/10/2015 0:00|11/11/2015 0:00|     32|Hyatt Union Squar...|Wednesday|            1|\n",
      "|10/11/2015 0:00|10/18/2015 0:00|      7|DoubleTree by Hil...|   Sunday|            1|\n",
      "|10/11/2015 0:00|10/27/2015 0:00|     16|The Carlyle A Ros...|  Tuesday|            1|\n",
      "|10/11/2015 0:00|10/27/2015 0:00|     16|The Carlyle A Ros...|  Tuesday|            2|\n",
      "|10/11/2015 0:00| 11/4/2015 0:00|     24|    Viceroy New York|Wednesday|            2|\n",
      "|10/12/2015 0:00|10/14/2015 0:00|      2|Hampton Inn Manha...|Wednesday|            2|\n",
      "|10/12/2015 0:00|10/15/2015 0:00|      3|    San Carlos Hotel| Thursday|            4|\n",
      "|10/12/2015 0:00|10/30/2015 0:00|     18|Manhattan NYC-an ...|   Friday|            2|\n",
      "|10/12/2015 0:00|11/14/2015 0:00|     33|Loews Regency New...| Saturday|            1|\n",
      "|10/12/2015 0:00| 11/6/2015 0:00|     25|Hampton Inn Madis...|   Friday|            3|\n",
      "|10/13/2015 0:00|10/21/2015 0:00|      8|Hilton Garden Inn...|Wednesday|            2|\n",
      "|10/13/2015 0:00|10/28/2015 0:00|     15|Newark Liberty In...|Wednesday|            1|\n",
      "|10/13/2015 0:00|10/29/2015 0:00|     16|Sheraton New York...| Thursday|            3|\n",
      "|10/13/2015 0:00|10/30/2015 0:00|     17|    Millenium Hilton|   Friday|            1|\n",
      "|10/13/2015 0:00|10/30/2015 0:00|     17|Residence Inn New...|   Friday|            2|\n",
      "|10/14/2015 0:00|10/17/2015 0:00|      3|Best Western Bowe...| Saturday|            2|\n",
      "+---------------+---------------+-------+--------------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "w = Window.partitionBy('Snapshot Date', 'Checkin Date', 'DayDiff', 'Hotel Name', 'WeekDay')\n",
    "\n",
    "# Take the max discount code from the grouped data\n",
    "hotelsBeforeClassifcation = hotels.withColumn('maxDis', f.max('DiscountPerc').over(w))\\\n",
    "    .where(f.col('DiscountPerc') == f.col('maxDis'))\\\n",
    "    .drop('maxDis')\\\n",
    "    .select('Snapshot Date', 'Checkin Date', 'DayDiff', 'Hotel Name', 'WeekDay', 'Discount Code')\n",
    "hotelsBeforeClassifcation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotelsBeforeClassifcation.coalesce(1).write.csv('Spark_Classification_Data', header=True)# Loading the data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotelsForClassifcation = spark.read.csv('Spark_Classification_Data',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-------+--------------------+---------+-------------+\n",
      "|  Snapshot Date|   Checkin Date|DayDiff|          Hotel Name|  WeekDay|Discount Code|\n",
      "+---------------+---------------+-------+--------------------+---------+-------------+\n",
      "|  1/1/2016 0:00| 1/13/2016 0:00|     12|Cassa Hotel 45th ...|Wednesday|            1|\n",
      "|  1/1/2016 0:00| 1/13/2016 0:00|     12|The New York EDITION|Wednesday|            3|\n",
      "|  1/1/2016 0:00| 1/29/2016 0:00|     28|Hampton Inn Manha...|   Friday|            4|\n",
      "| 10/1/2015 0:00| 10/2/2015 0:00|      1|Hilton Garden Inn...|   Friday|            4|\n",
      "|10/10/2015 0:00|11/11/2015 0:00|     32|Hyatt Union Squar...|Wednesday|            1|\n",
      "+---------------+---------------+-------+--------------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hotelsForClassifcation.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------+--------------------+---------+-------------+\n",
      "|Snapshot Date|   Checkin Date|DayDiff|          Hotel Name|  WeekDay|Discount Code|\n",
      "+-------------+---------------+-------+--------------------+---------+-------------+\n",
      "|         null| 1/13/2016 0:00|     12|Cassa Hotel 45th ...|Wednesday|            1|\n",
      "|         null| 1/13/2016 0:00|     12|The New York EDITION|Wednesday|            3|\n",
      "|         null| 1/29/2016 0:00|     28|Hampton Inn Manha...|   Friday|            4|\n",
      "|         null| 10/2/2015 0:00|      1|Hilton Garden Inn...|   Friday|            4|\n",
      "|         null|11/11/2015 0:00|     32|Hyatt Union Squar...|Wednesday|            1|\n",
      "+-------------+---------------+-------+--------------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,unix_timestamp,col\n",
    "spHotelsClassification = hotelsForClassifcation.withColumn('Snapshot Date', unix_timestamp(col('Snapshot Date'), \"dd/mm/yyyy hh:mm:ss a\"))\n",
    "spHotelsClassification.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+-------+--------------------+---------+-------------+\n",
      "|Snapshot Date|  Checkin Date|DayDiff|          Hotel Name|  WeekDay|Discount Code|\n",
      "+-------------+--------------+-------+--------------------+---------+-------------+\n",
      "|1/1/2016 0:00|1/13/2016 0:00|     12|Cassa Hotel 45th ...|Wednesday|            1|\n",
      "+-------------+--------------+-------+--------------------+---------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+--------------+-------+--------------------+---------+-------------+\n",
      "|Snapshot Date|  Checkin Date|DayDiff|          Hotel Name|  WeekDay|Discount Code|\n",
      "+-------------+--------------+-------+--------------------+---------+-------------+\n",
      "|1/1/2016 0:00|1/13/2016 0:00|     12|Cassa Hotel 45th ...|Wednesday|            1|\n",
      "+-------------+--------------+-------+--------------------+---------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+------------+-------+--------------------+---------+-------------+\n",
      "|Snapshot Date|Checkin Date|DayDiff|          Hotel Name|  WeekDay|Discount Code|\n",
      "+-------------+------------+-------+--------------------+---------+-------------+\n",
      "|   1451599260|  1452636060|     12|Cassa Hotel 45th ...|Wednesday|            1|\n",
      "|   1451599260|  1452636060|     12|The New York EDITION|Wednesday|            3|\n",
      "|   1451599260|  1454018460|     28|Hampton Inn Manha...|   Friday|            4|\n",
      "|   1420063800|  1420150200|      1|Hilton Garden Inn...|   Friday|            4|\n",
      "|   1420841400|  1420927860|     32|Hyatt Union Squar...|Wednesday|            1|\n",
      "+-------------+------------+-------+--------------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+------------+-------+--------------------+-------+-------------+\n",
      "|Snapshot Date|Checkin Date|DayDiff|          Hotel Name|WeekDay|Discount Code|\n",
      "+-------------+------------+-------+--------------------+-------+-------------+\n",
      "|   1451599260|  1452636060|     12|Cassa Hotel 45th ...|      4|            1|\n",
      "|   1451599260|  1452636060|     12|The New York EDITION|      4|            3|\n",
      "|   1451599260|  1454018460|     28|Hampton Inn Manha...|      6|            4|\n",
      "|   1420063800|  1420150200|      1|Hilton Garden Inn...|      6|            4|\n",
      "|   1420841400|  1420927860|     32|Hyatt Union Squar...|      4|            1|\n",
      "+-------------+------------+-------+--------------------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+------------+-------+-------+-------------+----------+\n",
      "|Snapshot Date|Checkin Date|DayDiff|WeekDay|Discount Code|Hotel Name|\n",
      "+-------------+------------+-------+-------+-------------+----------+\n",
      "|   1451599260|  1452636060|     12|      4|            1|      34.0|\n",
      "|   1451599260|  1452636060|     12|      4|            3|       9.0|\n",
      "|   1451599260|  1454018460|     28|      6|            4|      59.0|\n",
      "|   1420063800|  1420150200|      1|      6|            4|       1.0|\n",
      "|   1420841400|  1420927860|     32|      4|            1|      25.0|\n",
      "|   1420927800|  1421532600|      7|      1|            1|      81.0|\n",
      "|   1420927800|  1422310200|     16|      3|            1|       0.0|\n",
      "|   1420927800|  1422310200|     16|      3|            2|       0.0|\n",
      "|   1420927800|  1420323060|     24|      4|            2|       6.0|\n",
      "|   1421014200|  1421187000|      2|      4|            2|      42.0|\n",
      "|   1421014200|  1421273400|      3|      5|            4|     123.0|\n",
      "|   1421014200|  1422569400|     18|      6|            2|      74.0|\n",
      "|   1421014200|  1421187060|     33|      7|            1|       5.0|\n",
      "|   1421014200|  1420495860|     25|      6|            3|      35.0|\n",
      "|   1421100600|  1421791800|      8|      4|            2|     113.0|\n",
      "|   1421100600|  1422396600|     15|      4|            1|       3.0|\n",
      "|   1421100600|  1422483000|     16|      5|            3|      50.0|\n",
      "|   1421100600|  1422569400|     17|      6|            1|      30.0|\n",
      "|   1421100600|  1422569400|     17|      6|            2|      97.0|\n",
      "|   1421187000|  1421446200|      3|      7|            2|     108.0|\n",
      "+-------------+------------+-------+-------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hotelsForClassifcation.show(1)\n",
    "from pyspark.sql.functions import udf,unix_timestamp,col\n",
    "from pyspark.sql.types import IntegerType,StringType\n",
    "#convert non numeric data to numeric data for algorithem\n",
    "\n",
    "def weekToDay(x):\n",
    "    try:\n",
    "        return {\n",
    "        'Sunday': 1,\n",
    "        'Monday': 2,\n",
    "        'Tuesday': 3,\n",
    "        'Wednesday': 4,\n",
    "        'Thursday': 5,\n",
    "        'Friday': 6,\n",
    "        'Saturday': 7\n",
    "    }[x]\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "weekToDay_udf = udf(weekToDay, IntegerType())\n",
    "hotelsForClassifcation.show(1)\n",
    "spHotelsClassification = hotelsForClassifcation.withColumn('Snapshot Date', unix_timestamp(col('Snapshot Date'), \"mm/dd/yyyy\")). \\\n",
    "        withColumn('Checkin Date', unix_timestamp(hotelsForClassifcation['Checkin Date'], \"mm/dd/yyyy\"))\n",
    "spHotelsClassification.show(5)\n",
    "newCol = weekToDay_udf(spHotelsClassification[\"WeekDay\"])\n",
    "spHotelsClassification = spHotelsClassification.withColumn(\"WeekDay\", newCol)\n",
    "spHotelsClassification.show(5)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Hotel Name\", outputCol=\"Hotel Name_id\")\n",
    "indexed = indexer.fit(spHotelsClassification).transform(spHotelsClassification)\n",
    "final = indexed.drop(\"Hotel Name\").withColumnRenamed(\"Hotel Name_id\", \"Hotel Name\")\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([1420063740.0, 1420063800.0, 3.0, 30.0, 5.0]), Discount Code=4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "vAssembler = VectorAssembler(inputCols= ['Snapshot Date','Checkin Date','Hotel Name','DayDiff','WeekDay'],outputCol=\"features\")\n",
    "vector = vAssembler.transform(final)\n",
    "dataAndTarget = vector.select(\"features\", \"Discount Code\")\n",
    "\n",
    "train_data,test_data = dataAndTarget.randomSplit([0.7,0.3])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree test set accuracy = 0.6806817198719612\n"
     ]
    }
   ],
   "source": [
    "#DecisionTree\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(labelCol=\"Discount Code\", featuresCol=\"features\", maxBins=554, maxDepth=30)\n",
    "\n",
    "dtmodel = decisionTree.fit(train_data)\n",
    "\n",
    "dtpredictions = dtmodel.transform(test_data)\n",
    "dtevaluator = MulticlassClassificationEvaluator(labelCol=\"Discount Code\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = dtevaluator.evaluate(dtpredictions)\n",
    "print(\"Decision Tree test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes test set accuracy = 0.22582691697667043\n"
     ]
    }
   ],
   "source": [
    "#NaiveBayes\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "navBay = NaiveBayes(modelType=\"multinomial\", labelCol=\"Discount Code\")\n",
    "\n",
    "nbmodel = navBay.fit(train_data)\n",
    "\n",
    "nbpredictions = nbmodel.transform(test_data)\n",
    "nbevaluator = MulticlassClassificationEvaluator(labelCol=\"Discount Code\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = nbevaluator.evaluate(nbpredictions)\n",
    "print(\"Naive bayes test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results\n",
    "# The results we got were no different than the results we got in excercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 0.2329, 1: 0.2456, 2: 0.3592, 3: 0.1107, 4: 0.0516})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtmodel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive - Decision Tree\n",
      "0.6888548539114043 0.6972854400877434 0.6790084719171635 0.6393529101459469\n",
      "True Positive - Naive Bayes\n",
      "0.9102262016965127 0.007951741157115437 0.0018826482585503608 0.0\n",
      "False Poisitive - Decision Tree\n",
      "0.11611745389285578 0.1453067071115605 0.11466794075489728 0.05874439461883408\n",
      "False Poisitive - Naive Bayes\n",
      "0.9137805949062584 0.005645433097404786 0.0023889154323936935 0.0\n"
     ]
    }
   ],
   "source": [
    "# Let's Calculate TP and FP\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "decTree = dtpredictions.rdd.map(lambda x: (x[\"prediction\"], float(x[\"Discount Code\"])))\n",
    "navBay = nbpredictions.rdd.map(lambda x: (x[\"prediction\"], float(x[\"Discount Code\"])))\n",
    "\n",
    "metricsDT = MulticlassMetrics(decTree)\n",
    "metricsNB = MulticlassMetrics(navBay)\n",
    "\n",
    "print('True Positive - Decision Tree')\n",
    "print(metricsDT.truePositiveRate(1.0),metricsDT.truePositiveRate(2.0),metricsDT.truePositiveRate(3.0),metricsDT.truePositiveRate(4.0))\n",
    "print('True Positive - Naive Bayes')\n",
    "print(metricsNB.truePositiveRate(1.0),metricsNB.truePositiveRate(2.0),metricsNB.truePositiveRate(3.0),metricsNB.truePositiveRate(4.0))\n",
    "print('False Poisitive - Decision Tree')\n",
    "print(metricsDT.falsePositiveRate(1.0),metricsDT.falsePositiveRate(2.0),metricsDT.falsePositiveRate(3.0),metricsDT.falsePositiveRate(4.0))\n",
    "print('False Poisitive - Naive Bayes')\n",
    "print(metricsNB.falsePositiveRate(1.0),metricsNB.falsePositiveRate(2.0),metricsNB.falsePositiveRate(3.0),metricsNB.falsePositiveRate(4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Curve - Decision Tree - Discount code 1 is 0.7863687000092743\n",
      "ROC Curve - Naive Bayes - Discount code 1 is 0.49822280339512715\n",
      "ROC Curve - Decision Tree - Discount code 2 is 0.7759893664880915\n",
      "ROC Curve - Naive Bayes - Discount code 2 is 0.5011531540298554\n",
      "ROC Curve - Decision Tree - Discount code 3 is 0.7821702655811332\n",
      "ROC Curve - Naive Bayes - Discount code 3 is 0.4997468664130783\n",
      "ROC Curve - Decision Tree - Discount code 4 is 0.7903042577635563\n",
      "ROC Curve - Naive Bayes - Discount code 4 is 0.5\n"
     ]
    }
   ],
   "source": [
    "# Let's Calculate ROC Curve\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "def makeBinary(predictionAndTarget, index):\n",
    "    \n",
    "    first = 0.0\n",
    "    second = 0.0\n",
    "    if predictionAndTarget[0] == index:\n",
    "        first = 1.0\n",
    "    if predictionAndTarget[1] == index:\n",
    "        second = 1.0\n",
    "    return (first, second)\n",
    "\n",
    "#run to all the dicount code\n",
    "for index in range(1,5):\n",
    "    \n",
    "    currPredAndLabDT = decTree.map(lambda x: makeBinary(x, index))\n",
    "    currPredAndLabNB = navBay.map(lambda x: makeBinary(x, index))\n",
    "\n",
    "    # Instantiate metrics object\n",
    "    metricsDT = BinaryClassificationMetrics(currPredAndLabDT)\n",
    "    metricsNB = BinaryClassificationMetrics(currPredAndLabNB)\n",
    "    \n",
    "    # ROC\n",
    "    print(\"ROC Curve - Decision Tree - Discount code \" + str(index) + \" is \" + str(metricsDT.areaUnderROC))\n",
    "    print(\"ROC Curve - Naive Bayes - Discount code \" + str(index) + \" is \" + str(metricsNB.areaUnderROC))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
